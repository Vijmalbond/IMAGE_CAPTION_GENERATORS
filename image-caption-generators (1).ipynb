{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Generating a caption for a given image is a challenging problem in the deep learning domain. In this article, we will use different techniques of computer vision and NLP to recognize the context of an image and describe them in a natural language like English. we will build a working model of the image caption generator by using CNN.\n# #  For training our model I’m using ***Flickr8K*** **dataset. It consists of 8000 unique images and each image will be mapped to five different sentences which will describe the image****","metadata":{}},{"cell_type":"markdown","source":"# Step 1: Import the required libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nfrom tqdm.notebook import tqdm\n\nfrom tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img , img_to_array\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical, plot_model\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:03:21.618652Z","iopub.execute_input":"2022-04-03T17:03:21.619016Z","iopub.status.idle":"2022-04-03T17:03:26.539120Z","shell.execute_reply.started":"2022-04-03T17:03:21.618932Z","shell.execute_reply":"2022-04-03T17:03:26.538381Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"BASE_DIR= '/kaggle/input/flickr8k'\nWORKING_DIR= '/kaggle/working'","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:03:26.540938Z","iopub.execute_input":"2022-04-03T17:03:26.541241Z","iopub.status.idle":"2022-04-03T17:03:26.547348Z","shell.execute_reply.started":"2022-04-03T17:03:26.541177Z","shell.execute_reply":"2022-04-03T17:03:26.546500Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Extract Image Features","metadata":{}},{"cell_type":"code","source":"#load vgg16 model\nmodel =VGG16()\n#restructure the model\nmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n#summarize\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:03:26.550125Z","iopub.execute_input":"2022-04-03T17:03:26.551368Z","iopub.status.idle":"2022-04-03T17:03:45.452396Z","shell.execute_reply.started":"2022-04-03T17:03:26.551321Z","shell.execute_reply":"2022-04-03T17:03:45.451601Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#extract featurers from image\nfeatures= {}\ndirectory = os.path.join(BASE_DIR, 'Images')\n\nfor img_name in tqdm(os.listdir(directory)):\n    #load the image from file\n    img_path = directory + '/' + img_name\n    image = load_img(img_path, target_size=(224,224))\n    #convert image pixel to numpy array\n    image = img_to_array(image)\n    #reshape data for model\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    #preprocess image for vgg\n    image = preprocess_input(image)\n    #extract features\n    feature= model.predict(image, verbose=0)\n    #get image ID\n    image_id = img_name.split('.')[0]\n    #store feature\n    features[image_id] = feature\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:03:45.454278Z","iopub.execute_input":"2022-04-03T17:03:45.454558Z","iopub.status.idle":"2022-04-03T17:12:36.437757Z","shell.execute_reply.started":"2022-04-03T17:03:45.454520Z","shell.execute_reply":"2022-04-03T17:12:36.437080Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#store feature in pickle\npickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:36.440323Z","iopub.execute_input":"2022-04-03T17:12:36.440791Z","iopub.status.idle":"2022-04-03T17:12:36.769598Z","shell.execute_reply.started":"2022-04-03T17:12:36.440752Z","shell.execute_reply":"2022-04-03T17:12:36.768881Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#load features from pickle\nwith open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n    features= pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:36.771030Z","iopub.execute_input":"2022-04-03T17:12:36.771313Z","iopub.status.idle":"2022-04-03T17:12:36.933263Z","shell.execute_reply.started":"2022-04-03T17:12:36.771275Z","shell.execute_reply":"2022-04-03T17:12:36.932500Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Load the Captions Data","metadata":{}},{"cell_type":"code","source":"with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n    next(f)\n    captions_doc = f.read()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:36.935535Z","iopub.execute_input":"2022-04-03T17:12:36.935755Z","iopub.status.idle":"2022-04-03T17:12:36.979590Z","shell.execute_reply.started":"2022-04-03T17:12:36.935729Z","shell.execute_reply":"2022-04-03T17:12:36.978869Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# The format of our file is image and caption separated by a newline (“\\n”) i.e, it consists of the name of the image followed by a space and the description of the image in CSV format. Here we need to map the image to its descriptions by storing them in a dictionary.","metadata":{}},{"cell_type":"code","source":"#create mapping of image to captions\nmapping= {}\n#process lines\nfor line in tqdm(captions_doc.split('\\n')):\n    #split the line by comma(,)\n    tokens= line.split(',')\n    if len(line) < 2:\n        continue\n    image_id, caption = tokens[0], tokens[1:]\n    #remove extention from image ID\n    image_id = image_id.split('.')[0]\n    #convert caption list to string\n    caption =\" \".join(caption)\n    #create lis if needed\n    if image_id not in mapping:\n        mapping[image_id] = []\n    #store the caption\n    mapping[image_id].append(caption)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:36.980793Z","iopub.execute_input":"2022-04-03T17:12:36.981116Z","iopub.status.idle":"2022-04-03T17:12:37.120279Z","shell.execute_reply.started":"2022-04-03T17:12:36.981077Z","shell.execute_reply":"2022-04-03T17:12:37.119473Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"len(mapping)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:37.121588Z","iopub.execute_input":"2022-04-03T17:12:37.121988Z","iopub.status.idle":"2022-04-03T17:12:37.130365Z","shell.execute_reply.started":"2022-04-03T17:12:37.121951Z","shell.execute_reply":"2022-04-03T17:12:37.129652Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Pre-process text data","metadata":{}},{"cell_type":"markdown","source":"# One of the main steps in NLP is to remove noise so that the machine can detect the patterns easily in the text. Noise will be present in the form of special characters such as hashtags, punctuation and numbers. All of which are difficult for computers to understand if they are present in the text. So we need to remove these for better results. Additionally, you can also remove stop words and perform Stemming and Lemmatization by using NLTK library.","metadata":{}},{"cell_type":"code","source":"def clean(mapping):\n    for key, captions in mapping.items():\n        for i in range(len(captions)):\n            #take one caption at a time\n            caption= captions[i]\n            #preprocessing steps\n            caption = caption.lower()\n            #delet digit , special characters ,etc,...\n            caption=caption.replace('[^A-Za-z]','')\n            #delet additional spaces\n            caption = caption.replace('\\s+','')\n            #add start and end tags to the caption\n            caption= 'start' + \" \".join([word for word in caption.split() if len(word)>1]) + 'end'\n            captions[i] = caption\n            \n            ","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:37.131750Z","iopub.execute_input":"2022-04-03T17:12:37.132591Z","iopub.status.idle":"2022-04-03T17:12:37.140658Z","shell.execute_reply.started":"2022-04-03T17:12:37.132554Z","shell.execute_reply":"2022-04-03T17:12:37.139786Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Load the images\n# \n# Here we need to map the images in the training set to their corresponding descriptions which are present in our descriptions variable. Create a list of names of all training images and then create an empty dictionary and map the images to their descriptions using image name as key and a list of descriptions as its value. while mapping the descriptions add unique words at the beginning and end to identify the start and end of the sentence.","metadata":{}},{"cell_type":"code","source":"#before preprocess of text\nmapping['1000268201_693b08cb0e']","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:37.141973Z","iopub.execute_input":"2022-04-03T17:12:37.142701Z","iopub.status.idle":"2022-04-03T17:12:37.151882Z","shell.execute_reply.started":"2022-04-03T17:12:37.142665Z","shell.execute_reply":"2022-04-03T17:12:37.151051Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#preprocess the text\nclean(mapping)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:37.153333Z","iopub.execute_input":"2022-04-03T17:12:37.153652Z","iopub.status.idle":"2022-04-03T17:12:37.290879Z","shell.execute_reply.started":"2022-04-03T17:12:37.153618Z","shell.execute_reply":"2022-04-03T17:12:37.290204Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#after preprocess of text\nmapping['1000268201_693b08cb0e']","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:37.292631Z","iopub.execute_input":"2022-04-03T17:12:37.292822Z","iopub.status.idle":"2022-04-03T17:12:37.297407Z","shell.execute_reply.started":"2022-04-03T17:12:37.292798Z","shell.execute_reply":"2022-04-03T17:12:37.296722Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"all_captions = []\nfor key in mapping:\n    for caption in mapping[key]:\n        all_captions.append(caption)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:37.300584Z","iopub.execute_input":"2022-04-03T17:12:37.301372Z","iopub.status.idle":"2022-04-03T17:12:37.317182Z","shell.execute_reply.started":"2022-04-03T17:12:37.301255Z","shell.execute_reply":"2022-04-03T17:12:37.316497Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"len(all_captions)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:37.318300Z","iopub.execute_input":"2022-04-03T17:12:37.318638Z","iopub.status.idle":"2022-04-03T17:12:37.329066Z","shell.execute_reply.started":"2022-04-03T17:12:37.318564Z","shell.execute_reply":"2022-04-03T17:12:37.328375Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"all_captions[:10]","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:37.330459Z","iopub.execute_input":"2022-04-03T17:12:37.330734Z","iopub.status.idle":"2022-04-03T17:12:37.338333Z","shell.execute_reply.started":"2022-04-03T17:12:37.330699Z","shell.execute_reply":"2022-04-03T17:12:37.337567Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#tokenize the text\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(all_captions)\nvocab_size= len(tokenizer.word_index) +1","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:37.339931Z","iopub.execute_input":"2022-04-03T17:12:37.340315Z","iopub.status.idle":"2022-04-03T17:12:38.086758Z","shell.execute_reply.started":"2022-04-03T17:12:37.340278Z","shell.execute_reply":"2022-04-03T17:12:38.085983Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"vocab_size","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:38.087981Z","iopub.execute_input":"2022-04-03T17:12:38.088456Z","iopub.status.idle":"2022-04-03T17:12:38.093911Z","shell.execute_reply.started":"2022-04-03T17:12:38.088418Z","shell.execute_reply":"2022-04-03T17:12:38.093254Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#get maximum lenght of the caption available\nmax_length = max(len(caption.split()) for caption in all_captions)\nmax_length\n","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:38.095110Z","iopub.execute_input":"2022-04-03T17:12:38.095752Z","iopub.status.idle":"2022-04-03T17:12:38.150278Z","shell.execute_reply.started":"2022-04-03T17:12:38.095715Z","shell.execute_reply":"2022-04-03T17:12:38.149612Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Train test split","metadata":{}},{"cell_type":"code","source":"image_ids = list(mapping.keys())\nsplit =int(len(image_ids)*0.90)\ntrain = image_ids[:split]\ntest = image_ids[split:]","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:38.153928Z","iopub.execute_input":"2022-04-03T17:12:38.155824Z","iopub.status.idle":"2022-04-03T17:12:38.162968Z","shell.execute_reply.started":"2022-04-03T17:12:38.155786Z","shell.execute_reply":"2022-04-03T17:12:38.162349Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. Also, we have 8000 images and each image has 5 captions associated with it. It means we have 30000 examples for training our model. As there are more examples you can also use data generator for feeding input in the form of batches to our model rather than giving all at one time. For simplicity, I’m not using this here.\n# \n# Also, we are going to use an embedding matrix to store the relations between words in our vocabulary. An embedding matrix is a linear mapping of the original space to a real-valued space where entities will have meaningful relationships","metadata":{}},{"cell_type":"code","source":"#create data generator to get data in batch (avoid session crash)\ndef data_generator(data_keys, mapping, features, tokenizer,max_length, vocab_size, batch_size):\n    #loop over images\n    x1, x2 ,y =list(), list(), list()\n    n=0\n    while 1:\n        for key in data_keys:\n            n += 1\n            captions =mapping[key]\n            #process each caption\n            for caption in captions:\n                #encode the sequence\n                seq = tokenizer.texts_to_sequences([caption])[0]\n                #split the sequenc into x, y pairs\n                for i in range(1 ,len(seq)):\n                    #split into input and output pairs\n                    in_seq ,out_seq = seq[:i], seq[i]\n                    # pad input sequences\n                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n                    #encode output sequence\n                    out_seq = to_categorical([out_seq], num_classes = vocab_size)[0]\n                    \n                    \n                    #store the sequences\n                    x1.append(features[key][0])\n                    x2.append(in_seq)\n                    y.append(out_seq)\n            if n == batch_size:\n                x1, x2, y = np.array(x1), np.array(x2), np.array(y)\n                yield [x1, x2], y\n                x1, x2, y =list(), list(), list()\n                n=0\n                \n        \n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:38.164008Z","iopub.execute_input":"2022-04-03T17:12:38.167940Z","iopub.status.idle":"2022-04-03T17:12:38.183346Z","shell.execute_reply.started":"2022-04-03T17:12:38.167881Z","shell.execute_reply":"2022-04-03T17:12:38.182628Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Model Creation","metadata":{}},{"cell_type":"markdown","source":"\n# For defining the structure of our model, we will be using the Keras Model from Functional API. It has three major steps:\n\n# Processing the sequence from the text\n# Extracting the feature vector from the image\n# Decoding the output by concatenating the above two layers","metadata":{}},{"cell_type":"code","source":"#encoder model\n#image feature layers\ninput1= Input(shape=(4096,))\nfe1= Dropout(0.4)(input1)\nfe2= Dense(256, activation='relu')(fe1)\n\n#sequence feature layers\ninput2= Input(shape=(max_length,))\nse1= Embedding(vocab_size, 256, mask_zero=True)(input2)\nse2= Dropout(0.4)(se1)\nse3= LSTM(256)(se2)\n\n#decoder model\ndecoder1= add([fe2,se3])\ndecoder2= Dense(256, activation= 'relu')(decoder1)\noutputs= Dense(vocab_size, activation='softmax')(decoder2)\n\nmodel= Model(inputs=[input1, input2], outputs=outputs)\nmodel.compile(loss='categorical_crossentropy', optimizer= 'adam')\n\n#plot the model\nplot_model(model, show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:38.184612Z","iopub.execute_input":"2022-04-03T17:12:38.185009Z","iopub.status.idle":"2022-04-03T17:12:40.050622Z","shell.execute_reply.started":"2022-04-03T17:12:38.184977Z","shell.execute_reply":"2022-04-03T17:12:40.049506Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# For training our model I’m using Adam’s optimizer and loss function as categorical cross-entropy. I’m training the model for 50 epochs which will be enough for predicting the output. In case you have more computational power (no. of GPU’s) you can train it by decreasing batch size and increasing number of epochs.","metadata":{}},{"cell_type":"code","source":"#train the model\nepochs= 15\nbatch_size= 64\nsteps =len(train) // batch_size\n\nfor i in range(epochs):\n    #create data generator\n    generator= data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n    #fit for one epoch\n    model.fit(generator, epochs=1, steps_per_epoch= steps, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:12:40.051975Z","iopub.execute_input":"2022-04-03T17:12:40.052335Z","iopub.status.idle":"2022-04-03T17:30:33.416132Z","shell.execute_reply.started":"2022-04-03T17:12:40.052300Z","shell.execute_reply":"2022-04-03T17:30:33.415360Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"#save the model\nmodel.save(WORKING_DIR+ '/best_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:30:33.417415Z","iopub.execute_input":"2022-04-03T17:30:33.417629Z","iopub.status.idle":"2022-04-03T17:30:33.590652Z","shell.execute_reply.started":"2022-04-03T17:30:33.417603Z","shell.execute_reply":"2022-04-03T17:30:33.589715Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Generate Captions for the images","metadata":{}},{"cell_type":"code","source":"def idx_to_word(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:30:33.591975Z","iopub.execute_input":"2022-04-03T17:30:33.592395Z","iopub.status.idle":"2022-04-03T17:30:33.597662Z","shell.execute_reply.started":"2022-04-03T17:30:33.592349Z","shell.execute_reply":"2022-04-03T17:30:33.596620Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"#generate caption for an image\ndef predict_caption(model, image, tokenizer, max_lenght):\n    #add start tag for generation process\n    in_text = 'start'\n    #iterate over the max lenght of sequences\n    for i in range(max_length):\n        #encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        #pad the sequences\n        sequence = pad_sequences([sequence], max_length)\n        #predict next word\n        yhat = model.predict([image, sequence], verbose=0)\n        #get index with high probability\n        yhat = np.argmax(yhat)\n        #convert index to word\n        word = idx_to_word(yhat, tokenizer)\n        #stop if word not found\n        if word is None:\n            break\n        #append word as input for generating next word\n        in_text += \" \" + word\n        #stop if we reach and tag\n        if word == 'end':\n            break\n    return in_text\n        ","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:30:33.598925Z","iopub.execute_input":"2022-04-03T17:30:33.599306Z","iopub.status.idle":"2022-04-03T17:30:33.607775Z","shell.execute_reply.started":"2022-04-03T17:30:33.599273Z","shell.execute_reply":"2022-04-03T17:30:33.607087Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n#validate with test data\nactual, predicted = list(),list()\n\nfor key in tqdm(test):\n    #get actual caption\n    captions = mapping[key]\n    #predict the caption for image\n    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n    \n    #split into words\n    actual_captions=[caption.split() for caption in captions]\n    y_pred = y_pred.split()\n    \n    #append to the litst\n    actual.append(actual_captions)\n    predicted.append(y_pred)\n    \n\n#calculate BLEU score\nprint(\"BLUE-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\nprint(\"BLUE-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:30:33.608996Z","iopub.execute_input":"2022-04-03T17:30:33.609252Z","iopub.status.idle":"2022-04-03T17:38:42.751845Z","shell.execute_reply.started":"2022-04-03T17:30:33.609218Z","shell.execute_reply":"2022-04-03T17:38:42.751051Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Visualize the results","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\ndef generate_caption(image_name):\n    #Load the image\n    #image_name = \"1003163366_44323f5815.jpg\"\n    image_id= image_name.split('.')[0]\n    image_path= os.path.join(BASE_DIR, \"Images\", image_name)\n    image = Image.open(image_path)\n    captions= mapping[image_id]\n    print('---------------------Actual-------------------')\n    for caption in captions:\n        print(caption)\n    #predict the caption\n    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n    print('----------------------Predicted----------------------')\n    print(y_pred)\n    plt.imshow(image)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:38:42.753420Z","iopub.execute_input":"2022-04-03T17:38:42.753698Z","iopub.status.idle":"2022-04-03T17:38:42.761531Z","shell.execute_reply.started":"2022-04-03T17:38:42.753660Z","shell.execute_reply":"2022-04-03T17:38:42.759720Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"generate_caption(\"1020651753_06077ec457.jpg\")","metadata":{"execution":{"iopub.status.busy":"2022-04-03T17:38:42.762813Z","iopub.execute_input":"2022-04-03T17:38:42.763344Z","iopub.status.idle":"2022-04-03T17:38:43.665580Z","shell.execute_reply.started":"2022-04-03T17:38:42.763303Z","shell.execute_reply":"2022-04-03T17:38:43.661790Z"},"trusted":true},"execution_count":29,"outputs":[]}]}